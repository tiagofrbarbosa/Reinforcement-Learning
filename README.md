# ğŸ¤– Projeto Integrado â€“ Aprendizado por ReforÃ§o Aplicado ao Mercado Financeiro

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](LICENSE)

Este projeto foi desenvolvido como parte do **MBA em CiÃªncia de Dados e InteligÃªncia Artificial** da **FIAP** pela turma **9DTSR**, com foco em aplicar tÃ©cnicas de **Reinforcement Learning (RL)** para decisÃµes automatizadas de compra e venda de aÃ§Ãµes.

Utilizamos **Deep Q-Network (DQN)** para treinar um agente capaz de operar no mercado com base em dados histÃ³ricos dos ativos **VALE3.SA**, **PETR4.SA** e **BRFS3.SA**.

---

## ğŸ¯ Objetivo

Desenvolver um agente de RL capaz de **decidir entre comprar, vender ou manter** aÃ§Ãµes com base em estados do mercado financeiro, visando:

- Maximizar retorno acumulado
- Reduzir risco com decisÃµes otimizadas
- Automatizar estratÃ©gias de investimento com IA

---

## ğŸ“Š Dados Utilizados

Os dados foram obtidos por meio da biblioteca `yfinance`, com o perÃ­odo de **01/01/2022 a 31/12/2024** e representam os preÃ§os ajustados dos ativos:

- VALE3.SA (Vale)
- PETR4.SA (Petrobras)
- BRFS3.SA (Brasil Foods)

---

## ğŸ§  DefiniÃ§Ã£o do Ambiente de RL

| Componente | DescriÃ§Ã£o |
|------------|-----------|
| **Estados** | PreÃ§o atual dos ativos, posiÃ§Ãµes em carteira, saldo disponÃ­vel, indicadores tÃ©cnicos |
| **AÃ§Ãµes**   | Comprar, Vender ou Manter (por ativo, com base em saldo/estoque) |
| **Recompensas** | Lucro/prejuÃ­zo obtido apÃ³s cada aÃ§Ã£o, baseado na variaÃ§Ã£o patrimonial |

---

## ğŸ§ª Treinamento do Agente

Utilizamos o algoritmo **DQN (Deep Q-Network)** do pacote `stable-baselines3`, com arquitetura MLP (Multilayer Perceptron) e os seguintes parÃ¢metros principais:

- **PolÃ­tica:** `MlpPolicy`
- **Total timesteps:** `50.000`
- **ExploraÃ§Ã£o final (epsilon):** `0.05`

Durante o treinamento, o agente:

- Aprende a maximizar as recompensas acumuladas
- Explora diferentes estratÃ©gias atÃ© consolidar sua polÃ­tica Ã³tima
- Mostra sinais de aprendizado com aumento consistente do patrimÃ´nio

---

## ğŸ“ˆ Resultados

- O agente partiu com **R$10.000** e alcanÃ§ou um patrimÃ´nio superior a **R$13.500**
- Demonstrou aprendizado real, com decisÃµes eficazes e sustentadas
- Foi avaliado com base em mÃ©tricas como **retorno total**, **Ã­ndice de Sharpe** e **mÃ¡ximo drawdown**

---

## ğŸ” Principais ConclusÃµes

- **RL Ã© promissor** na automaÃ§Ã£o de investimentos, com capacidade adaptativa e tomada de decisÃ£o autÃ´noma
- Ã‰ fundamental aplicar tÃ©cnicas de **validaÃ§Ã£o robusta** e **controle de risco**
- Ajustes finos de hiperparÃ¢metros e maior volume de dados podem melhorar a estabilidade e desempenho do agente

---

## ğŸ› ï¸ Tecnologias e Bibliotecas

- Python 3.11+
- [stable-baselines3](https://github.com/DLR-RM/stable-baselines3)
- yfinance
- pandas / numpy / matplotlib

---

## ğŸ‘¥ Autores

- **JÃ©ssica Portela de Castro** â€“ RM: 359735  
- **Tiago Freire Barbosa** â€“ RM: 358404

Projeto desenvolvido para a disciplina de **Aprendizado por ReforÃ§o** no MBA FIAP â€“ Quantum Finance.

---

## ğŸ“œ LicenÃ§a

DistribuÃ­do sob a licenÃ§a [MIT](LICENSE).  
Uso livre para fins acadÃªmicos, pessoais e de pesquisa.

---

## ğŸ“š ReferÃªncia

> TATSAT, Hariom; PURI, Sahil; LOOKABAUGH, Brad. *Blueprints de aprendizado de mÃ¡quina e ciÃªncia de dados para finanÃ§as: desenvolvendo desde estratÃ©gias de trades atÃ© robÃ´s advisors com Python*. Rio de Janeiro: Alta Books, 2021.

---

## ğŸ”– Tags

`#ReinforcementLearning` `#DQN` `#FIAP` `#MercadoFinanceiro` `#QuantumFinance` `#MachineLearning` `#StableBaselines3` `#YFinance` `#OpenSource`
